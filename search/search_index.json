{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biobank Tools \u00b6 Documentation: https://altaf-ali.github.io/biobank-tools GitHub: https://github.com/altaf-ali/biobank-tools PyPI: https://pypi.org/project/biobank-tools/ Features \u00b6 The Biobank Tools package provides simple, fast, and efficient access to UK Biobank data. Once you've downloaded and extracted the UK Biobank data to comma or tab separated files, you can use Biobank Tools to convert the data to a format that's better suited for searching and filtering than plain text files. Internally, Biobank Tools convert the data to Apache Parquet format for optimized column-wise access. Requirements \u00b6 Biobank Tools require Python 3.8 or above. Credits \u00b6 This package was created with Cookiecutter and the altaf-ali/cookiecutter-pypackage project template.","title":"Introduction"},{"location":"#biobank-tools","text":"Documentation: https://altaf-ali.github.io/biobank-tools GitHub: https://github.com/altaf-ali/biobank-tools PyPI: https://pypi.org/project/biobank-tools/","title":"Biobank Tools"},{"location":"#features","text":"The Biobank Tools package provides simple, fast, and efficient access to UK Biobank data. Once you've downloaded and extracted the UK Biobank data to comma or tab separated files, you can use Biobank Tools to convert the data to a format that's better suited for searching and filtering than plain text files. Internally, Biobank Tools convert the data to Apache Parquet format for optimized column-wise access.","title":"Features"},{"location":"#requirements","text":"Biobank Tools require Python 3.8 or above.","title":"Requirements"},{"location":"#credits","text":"This package was created with Cookiecutter and the altaf-ali/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Biobank Dataset class. Dataset \u00b6 Biobank Dataset class. __init__ ( self ) special \u00b6 Constructor. Source code in biobank/dataset.py def __init__ ( self ): \"\"\"Constructor.\"\"\" path = settings . path . absolute () self . path = path / self . filename self . dictionary = Dictionary ( path ) delete ( self ) \u00b6 Delete the dataset. Returns: Type Description None None Source code in biobank/dataset.py def delete ( self ) -> None : \"\"\"Delete the dataset. Returns: None \"\"\" if self . path . is_file (): self . path . unlink () else : shutil . rmtree ( str ( self . path )) import_dataset ( self , path , dictionary ) \u00b6 Import a dataset. Parameters: Name Type Description Default path URL or local path of dataset to import required Returns: Type Description None None Source code in biobank/dataset.py def import_dataset ( self , path , dictionary ) -> None : \"\"\"Import a dataset. Args: path: URL or local path of dataset to import Returns: None \"\"\" self . dictionary . load ( dictionary , download = True ) with ProgressBar (): import_manager = ImportManager () data , schema = import_manager . import_dataset ( self . dictionary , path ) with ProgressBar (): self . save ( data , schema ) load ( self , ** kwargs ) \u00b6 Loads a previously imported Biobank dataset. Parameters: Name Type Description Default **kwargs Dictionary of keyword arguments {} Returns: Type Description DataFrame A Dask DataFrame object Source code in biobank/dataset.py def load ( self , ** kwargs ) -> dd . DataFrame : \"\"\"Loads a previously imported Biobank dataset. Args: **kwargs: Dictionary of keyword arguments Returns: A Dask DataFrame object \"\"\" return dd . read_parquet ( str ( self . path ), ** kwargs ) load_metadata ( self ) \u00b6 Loads metadata for the Biobank dataset. Returns: Type Description FileMetaData A FileMetadata object Source code in biobank/dataset.py def load_metadata ( self ) -> pq . FileMetaData : \"\"\"Loads metadata for the Biobank dataset. Returns: A FileMetadata object \"\"\" return pq . read_metadata ( self . path / \"_common_metadata\" ) save ( self , data , schema , ** kwargs ) \u00b6 Saves a Biobank dataset. Parameters: Name Type Description Default data Biobank dataset as a Dask Dataframe required schema Parquet schema required **kwargs Dictionary of keyword arguments {} Returns: Type Description None None Source code in biobank/dataset.py def save ( self , data , schema , ** kwargs ) -> None : \"\"\"Saves a Biobank dataset. Args: data: Biobank dataset as a Dask Dataframe schema: Parquet schema **kwargs: Dictionary of keyword arguments Returns: None \"\"\" print ( f \"saving dataset to { self . path } \" ) data . to_parquet ( self . path , schema = schema , compression = \"snappy\" , engine = \"pyarrow\" ) select ( self , fields = None , limit = None ) \u00b6 Select specific fields from the Biobank dataset. Parameters: Name Type Description Default fields List of fields to select None limit Number of rows to select None Returns: Type Description DataFrame A Pandas DataFrame Source code in biobank/dataset.py def select ( self , fields = None , limit = None ) -> pd . DataFrame : \"\"\"Select specific fields from the Biobank dataset. Args: fields: List of fields to select limit: Number of rows to select Returns: A Pandas DataFrame \"\"\" if fields : fields = self . match_fields ( fields ) if not len ( fields ): return pd . DataFrame () else : fields = None with ProgressBar (): dataset = self . load ( columns = fields , use_threads = True ) if limit : dataset = dataset . loc [ dataset . index . isin ( dataset . index . head ( limit )) ] dataset = dataset . replace ( to_replace = { col : { np . nan : \"\" } for col in dataset . select_dtypes ( [ np . float64 , np . datetime64 , object ] ) . columns } ) dataset = dataset . compute () return dataset Dictionary class for managing data dictionary. Dictionary \u00b6 Dictionary class for managing data dictionary. __init__ ( self , path ) special \u00b6 Constructor. Parameters: Name Type Description Default path Path Directory to store the dictionary required Source code in biobank/dictionary.py def __init__ ( self , path : Path ): \"\"\"Constructor. Args: path: Directory to store the dictionary \"\"\" self . _fields = None self . path = path / self . filename download ( self , path = None ) \u00b6 Download dictionary from URL. Returns: Type Description None None Source code in biobank/dictionary.py def download ( self , path = None ) -> None : \"\"\"Download dictionary from URL. Returns: None \"\"\" if not path : path = settings . dictionary . url self . path . parent . mkdir ( parents = True , exist_ok = True ) with ProgressBar (): print ( f \"loading dictionary from { path } \" ) dictionary = dd . read_table ( path ) dictionary = dictionary . compute () dictionary . to_parquet ( self . path ) filter ( self , fields , search ) \u00b6 Filter dictionary. Parameters: Name Type Description Default fields List[str] required search str required Returns: Type Description DataFrame None Source code in biobank/dictionary.py def filter ( self , fields : List [ str ], search : str ) -> pd . DataFrame : \"\"\"Filter dictionary. Args: fields: search: Returns: None \"\"\" dictionary = self . load () field_ids = set ( map ( self . get_field_id , fields )) dictionary = dictionary [ dictionary . FieldID . isin ( field_ids )] if search : dictionary = dictionary [ dictionary . Field . str . contains ( search , case = False ) ] return dictionary get_field_id ( self , field ) \u00b6 Get field ID. Parameters: Name Type Description Default field str Field name required Returns: Type Description str Field ID as str Source code in biobank/dictionary.py def get_field_id ( self , field : str ) -> str : \"\"\"Get field ID. Args: field: Field name Returns: Field ID as str \"\"\" return field . split ( \"-\" )[ 0 ] get_pandas_dtype ( self , field ) \u00b6 Get Pandas type for a field. Parameters: Name Type Description Default field Name of field required Returns: Type Description Any Pandas dtype Source code in biobank/dictionary.py def get_pandas_dtype ( self , field ) -> Any : \"\"\"Get Pandas type for a field. Args: field: Name of field Returns: Pandas dtype \"\"\" field_type = self . get_type ( field ) if not field_type : return None if field_type == pa . int64 (): return \"Int64\" # use pandas nullable integer type return field_type . to_pandas_dtype () get_type ( self , field ) \u00b6 Get Arrow data type for a field. Parameters: Name Type Description Default field Name of field required Returns: Type Description DataType Arrow field type Source code in biobank/dictionary.py def get_type ( self , field ) -> pa . DataType : \"\"\"Get Arrow data type for a field. Args: field: Name of field Returns: Arrow field type \"\"\" field = self . get_field_id ( field ) if field in self . fields . index : return self . fields . loc [ field ] . Type","title":"Reference"},{"location":"api/#biobank.dataset.Dataset","text":"Biobank Dataset class.","title":"Dataset"},{"location":"api/#biobank.dataset.Dataset.__init__","text":"Constructor. Source code in biobank/dataset.py def __init__ ( self ): \"\"\"Constructor.\"\"\" path = settings . path . absolute () self . path = path / self . filename self . dictionary = Dictionary ( path )","title":"__init__()"},{"location":"api/#biobank.dataset.Dataset.delete","text":"Delete the dataset. Returns: Type Description None None Source code in biobank/dataset.py def delete ( self ) -> None : \"\"\"Delete the dataset. Returns: None \"\"\" if self . path . is_file (): self . path . unlink () else : shutil . rmtree ( str ( self . path ))","title":"delete()"},{"location":"api/#biobank.dataset.Dataset.import_dataset","text":"Import a dataset. Parameters: Name Type Description Default path URL or local path of dataset to import required Returns: Type Description None None Source code in biobank/dataset.py def import_dataset ( self , path , dictionary ) -> None : \"\"\"Import a dataset. Args: path: URL or local path of dataset to import Returns: None \"\"\" self . dictionary . load ( dictionary , download = True ) with ProgressBar (): import_manager = ImportManager () data , schema = import_manager . import_dataset ( self . dictionary , path ) with ProgressBar (): self . save ( data , schema )","title":"import_dataset()"},{"location":"api/#biobank.dataset.Dataset.load","text":"Loads a previously imported Biobank dataset. Parameters: Name Type Description Default **kwargs Dictionary of keyword arguments {} Returns: Type Description DataFrame A Dask DataFrame object Source code in biobank/dataset.py def load ( self , ** kwargs ) -> dd . DataFrame : \"\"\"Loads a previously imported Biobank dataset. Args: **kwargs: Dictionary of keyword arguments Returns: A Dask DataFrame object \"\"\" return dd . read_parquet ( str ( self . path ), ** kwargs )","title":"load()"},{"location":"api/#biobank.dataset.Dataset.load_metadata","text":"Loads metadata for the Biobank dataset. Returns: Type Description FileMetaData A FileMetadata object Source code in biobank/dataset.py def load_metadata ( self ) -> pq . FileMetaData : \"\"\"Loads metadata for the Biobank dataset. Returns: A FileMetadata object \"\"\" return pq . read_metadata ( self . path / \"_common_metadata\" )","title":"load_metadata()"},{"location":"api/#biobank.dataset.Dataset.save","text":"Saves a Biobank dataset. Parameters: Name Type Description Default data Biobank dataset as a Dask Dataframe required schema Parquet schema required **kwargs Dictionary of keyword arguments {} Returns: Type Description None None Source code in biobank/dataset.py def save ( self , data , schema , ** kwargs ) -> None : \"\"\"Saves a Biobank dataset. Args: data: Biobank dataset as a Dask Dataframe schema: Parquet schema **kwargs: Dictionary of keyword arguments Returns: None \"\"\" print ( f \"saving dataset to { self . path } \" ) data . to_parquet ( self . path , schema = schema , compression = \"snappy\" , engine = \"pyarrow\" )","title":"save()"},{"location":"api/#biobank.dataset.Dataset.select","text":"Select specific fields from the Biobank dataset. Parameters: Name Type Description Default fields List of fields to select None limit Number of rows to select None Returns: Type Description DataFrame A Pandas DataFrame Source code in biobank/dataset.py def select ( self , fields = None , limit = None ) -> pd . DataFrame : \"\"\"Select specific fields from the Biobank dataset. Args: fields: List of fields to select limit: Number of rows to select Returns: A Pandas DataFrame \"\"\" if fields : fields = self . match_fields ( fields ) if not len ( fields ): return pd . DataFrame () else : fields = None with ProgressBar (): dataset = self . load ( columns = fields , use_threads = True ) if limit : dataset = dataset . loc [ dataset . index . isin ( dataset . index . head ( limit )) ] dataset = dataset . replace ( to_replace = { col : { np . nan : \"\" } for col in dataset . select_dtypes ( [ np . float64 , np . datetime64 , object ] ) . columns } ) dataset = dataset . compute () return dataset Dictionary class for managing data dictionary.","title":"select()"},{"location":"api/#biobank.dictionary.Dictionary","text":"Dictionary class for managing data dictionary.","title":"Dictionary"},{"location":"api/#biobank.dictionary.Dictionary.__init__","text":"Constructor. Parameters: Name Type Description Default path Path Directory to store the dictionary required Source code in biobank/dictionary.py def __init__ ( self , path : Path ): \"\"\"Constructor. Args: path: Directory to store the dictionary \"\"\" self . _fields = None self . path = path / self . filename","title":"__init__()"},{"location":"api/#biobank.dictionary.Dictionary.download","text":"Download dictionary from URL. Returns: Type Description None None Source code in biobank/dictionary.py def download ( self , path = None ) -> None : \"\"\"Download dictionary from URL. Returns: None \"\"\" if not path : path = settings . dictionary . url self . path . parent . mkdir ( parents = True , exist_ok = True ) with ProgressBar (): print ( f \"loading dictionary from { path } \" ) dictionary = dd . read_table ( path ) dictionary = dictionary . compute () dictionary . to_parquet ( self . path )","title":"download()"},{"location":"api/#biobank.dictionary.Dictionary.filter","text":"Filter dictionary. Parameters: Name Type Description Default fields List[str] required search str required Returns: Type Description DataFrame None Source code in biobank/dictionary.py def filter ( self , fields : List [ str ], search : str ) -> pd . DataFrame : \"\"\"Filter dictionary. Args: fields: search: Returns: None \"\"\" dictionary = self . load () field_ids = set ( map ( self . get_field_id , fields )) dictionary = dictionary [ dictionary . FieldID . isin ( field_ids )] if search : dictionary = dictionary [ dictionary . Field . str . contains ( search , case = False ) ] return dictionary","title":"filter()"},{"location":"api/#biobank.dictionary.Dictionary.get_field_id","text":"Get field ID. Parameters: Name Type Description Default field str Field name required Returns: Type Description str Field ID as str Source code in biobank/dictionary.py def get_field_id ( self , field : str ) -> str : \"\"\"Get field ID. Args: field: Field name Returns: Field ID as str \"\"\" return field . split ( \"-\" )[ 0 ]","title":"get_field_id()"},{"location":"api/#biobank.dictionary.Dictionary.get_pandas_dtype","text":"Get Pandas type for a field. Parameters: Name Type Description Default field Name of field required Returns: Type Description Any Pandas dtype Source code in biobank/dictionary.py def get_pandas_dtype ( self , field ) -> Any : \"\"\"Get Pandas type for a field. Args: field: Name of field Returns: Pandas dtype \"\"\" field_type = self . get_type ( field ) if not field_type : return None if field_type == pa . int64 (): return \"Int64\" # use pandas nullable integer type return field_type . to_pandas_dtype ()","title":"get_pandas_dtype()"},{"location":"api/#biobank.dictionary.Dictionary.get_type","text":"Get Arrow data type for a field. Parameters: Name Type Description Default field Name of field required Returns: Type Description DataType Arrow field type Source code in biobank/dictionary.py def get_type ( self , field ) -> pa . DataType : \"\"\"Get Arrow data type for a field. Args: field: Name of field Returns: Arrow field type \"\"\" field = self . get_field_id ( field ) if field in self . fields . index : return self . fields . loc [ field ] . Type","title":"get_type()"},{"location":"changelog/","text":"Changelog \u00b6 0.1.0 (2022-02-24) \u00b6 First release on PyPI.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#010-2022-02-24","text":"First release on PyPI.","title":"0.1.0 (2022-02-24)"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/altaf-ali/biobank-tools/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 Biobank Tools could always use more documentation, whether as part of the official Biobank Tools docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/altaf-ali/biobank-tools/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up biobank-tools for local development. Fork the biobank-tools repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/biobank-tools.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9, and above. Check https://github.com/altaf-ali/biobank-tools/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_biobank.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/altaf-ali/biobank-tools/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"Biobank Tools could always use more documentation, whether as part of the official Biobank Tools docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/altaf-ali/biobank-tools/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up biobank-tools for local development. Fork the biobank-tools repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/biobank-tools.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9, and above. Check https://github.com/altaf-ali/biobank-tools/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_biobank.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install Biobank Tools, run this command in your terminal: $ pip install biobank-tools ---> 100% Successfully installed biobank-tools This is the preferred method to install Biobank Tools, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for Biobank Tools can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/altaf-ali/biobank-tools Or download the tarball : $ curl -OJL https://github.com/altaf-ali/biobank-tools/tarball/master Once you have a copy of the source, you can install it with: $ pip install . Verify installation \u00b6 Verify that Biobank Tools are installed correctly by running the biobank command. $ biobank --help Usage: biobank [OPTIONS] COMMAND [ARGS]... Options: --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: exclude Exclude from biobank dataset. fields Show dataset fields. import Import biobank dataset. select Select fields from a dataset.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install Biobank Tools, run this command in your terminal: $ pip install biobank-tools ---> 100% Successfully installed biobank-tools This is the preferred method to install Biobank Tools, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for Biobank Tools can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/altaf-ali/biobank-tools Or download the tarball : $ curl -OJL https://github.com/altaf-ali/biobank-tools/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"installation/#verify-installation","text":"Verify that Biobank Tools are installed correctly by running the biobank command. $ biobank --help Usage: biobank [OPTIONS] COMMAND [ARGS]... Options: --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: exclude Exclude from biobank dataset. fields Show dataset fields. import Import biobank dataset. select Select fields from a dataset.","title":"Verify installation"},{"location":"tutorial/","text":"Tutorial \u00b6 This tutorial walks you through the following stages of working with UK biobank data: Importing UK biobank data and converting to Parquet format. Filtering and exporting the dataset from the command line. Loading and filtering the dataset from Python. Importing dataset \u00b6 This tutorial assumes that you have access to UK Biobank data and have downloaded and extracted the data to either comma separated (.CSV) or tab separated (.TSV) format. Use the biobank import command to import UK Biobank data. $ biobank import --help Usage: biobank import [OPTIONS] FILENAME Import biobank dataset. Arguments: FILENAME [required] Options: --dictionary PATH --force / --no-force [default: no-force] --help Show this message and exit. Depending on the size of the dataset, the import command could take several minutes. $ biobank import ukb12345.csv loading dictionary from https://biobank.ctsu.ox.ac.uk ---> 100% importing ukb12345.csv ---> 100% saving dataset to .biobank/dataset.parquet ---> 100% Working with data dictionary \u00b6 With Biobank Tools, you can search the data dictionary for fields by ID or by search terms. If you already know the field ID, you can check whether the field is available in your dataset. $ biobank fields 20006 20007 FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... You can get the same results using regular expressions as well. $ biobank fields \"2000[67]\" FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... If you don't know the field ID, you can use a search term to find the field IDs. $ biobank fields --search cancer FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... 851 20008 Continuous micrometres Interpolated Year when non-cancer illness firs... 852 20009 Continuous micrometres Interpolated Age of participant when non-cance... Finally, you can combine field ID and search terms to further narrow the results. $ biobank fields 200 .. --search age FieldID ValueType Units Field 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... 852 20009 Continuous micrometres Interpolated Age of participant when non-cance... 854 20011 Continuous micrometres Interpolated Age of participant when operation... Excluding withdrawn participants \u00b6 When you receive a list of withdrawn participants, you can exclude them from the dataset permanently. $ biobank exclude data/w12345_*.csv removing 20 records from 2 files: - data/w12345_20220101.csv - data/w12345_20220201.csv saving dataset to .biobank/dataset.parquet ---> 100% Extracting data \u00b6 There are two ways to work with dataset created by Biobank Tools. You can export the data to a .CSV file You can load the dataset directly in Python Exporting to .CSV \u00b6 With biobank select command, you can export a subset of the dataset that only contains the fields you're interested in. $ biobank select 2000 . --output data/ukb_cancer.csv ---> 100% writing data/ukb_cancer.csv If you omit the --output option, the biobank select command will output the results to the screen instead. $ biobank select \"2000[67]\" [########################################] | 100% Completed | 9.8s 20006-0.0 20006-0.1 20006-0.2 20006-0.3 20006-0.4 20006-0.5 20006-1.0 20006-1.1 ... 20007-1.3 20007-2.0 20007-2.1 20007-2.2 20007-2.3 20007-2.4 20007-3.0 20007-3.1 eid ... 1000016 1951.771450 1959.965254 1959.758862 2108.331238 1793.185997 2052.090334 ... 1000048 1927.954080 1780.621043 1768.346392 2166.976559 1092.823152 1958.248591 ... 57.135394 52.331757 49.393116 64.879196 68.577423 51.484781 66.384343 1000057 1783.957575 2055.652053 2055.706534 1854.930873 1398.633216 1942.852454 ... 1000059 2036.818137 2222.614293 2239.943216 1932.932354 2801.628978 1920.073145 ... 1000063 2037.248163 2142.236323 2146.492165 2092.512233 1534.692268 1844.065170 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9999968 2175.387378 2169.194698 2177.700380 1996.962135 1942.853501 1916.324173 ... 9999979 1994.938137 1830.922253 1823.217556 1993.885899 1436.625129 1899.035208 ... 9999980 1958.735897 2086.810754 2103.205981 2000.599709 3291.533833 1963.010565 ... 9999986 2002.481976 2039.945909 2038.244240 2068.425323 1250.346010 2066.319885 ... 9999992 1977.196447 1949.820874 1951.181053 1881.749111 2033.254502 1808.468535 ... [599995 rows x 34 columns] Loading dataset in Python \u00b6 You can load the dataset in Python and apply the same field filters that you would at the command line. from biobank import Dataset dataset = Dataset () cancer_data = dataset . select ( fields = [ \"2000[67]\" ]) print ( cancer_data ) 20006-0.0 20006-0.1 20006-0.2 20006-0.3 20006-0.4 20006-0.5 20006-1.0 20006-1.1 ... 20007-1.3 20007-2.0 20007-2.1 20007-2.2 20007-2.3 20007-2.4 20007-3.0 20007-3.1 eid ... 1000016 1951.771450 1959.965254 1959.758862 2108.331238 1793.185997 2052.090334 ... 1000048 1927.954080 1780.621043 1768.346392 2166.976559 1092.823152 1958.248591 ... 57.135394 52.331757 49.393116 64.879196 68.577423 51.484781 66.384343 1000057 1783.957575 2055.652053 2055.706534 1854.930873 1398.633216 1942.852454 ... 1000059 2036.818137 2222.614293 2239.943216 1932.932354 2801.628978 1920.073145 ... 1000063 2037.248163 2142.236323 2146.492165 2092.512233 1534.692268 1844.065170 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9999968 2175.387378 2169.194698 2177.700380 1996.962135 1942.853501 1916.324173 ... 9999979 1994.938137 1830.922253 1823.217556 1993.885899 1436.625129 1899.035208 ... 9999980 1958.735897 2086.810754 2103.205981 2000.599709 3291.533833 1963.010565 ... 9999986 2002.481976 2039.945909 2038.244240 2068.425323 1250.346010 2066.319885 ... 9999992 1977.196447 1949.820874 1951.181053 1881.749111 2033.254502 1808.468535 ... [599995 rows x 34 columns]","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This tutorial walks you through the following stages of working with UK biobank data: Importing UK biobank data and converting to Parquet format. Filtering and exporting the dataset from the command line. Loading and filtering the dataset from Python.","title":"Tutorial"},{"location":"tutorial/#importing-dataset","text":"This tutorial assumes that you have access to UK Biobank data and have downloaded and extracted the data to either comma separated (.CSV) or tab separated (.TSV) format. Use the biobank import command to import UK Biobank data. $ biobank import --help Usage: biobank import [OPTIONS] FILENAME Import biobank dataset. Arguments: FILENAME [required] Options: --dictionary PATH --force / --no-force [default: no-force] --help Show this message and exit. Depending on the size of the dataset, the import command could take several minutes. $ biobank import ukb12345.csv loading dictionary from https://biobank.ctsu.ox.ac.uk ---> 100% importing ukb12345.csv ---> 100% saving dataset to .biobank/dataset.parquet ---> 100%","title":"Importing dataset"},{"location":"tutorial/#working-with-data-dictionary","text":"With Biobank Tools, you can search the data dictionary for fields by ID or by search terms. If you already know the field ID, you can check whether the field is available in your dataset. $ biobank fields 20006 20007 FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... You can get the same results using regular expressions as well. $ biobank fields \"2000[67]\" FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... If you don't know the field ID, you can use a search term to find the field IDs. $ biobank fields --search cancer FieldID ValueType Units Field 849 20006 Continuous micrometres Interpolated Year when cancer first diagnosed 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... 851 20008 Continuous micrometres Interpolated Year when non-cancer illness firs... 852 20009 Continuous micrometres Interpolated Age of participant when non-cance... Finally, you can combine field ID and search terms to further narrow the results. $ biobank fields 200 .. --search age FieldID ValueType Units Field 850 20007 Continuous micrometres Interpolated Age of participant when cancer fi... 852 20009 Continuous micrometres Interpolated Age of participant when non-cance... 854 20011 Continuous micrometres Interpolated Age of participant when operation...","title":"Working with data dictionary"},{"location":"tutorial/#excluding-withdrawn-participants","text":"When you receive a list of withdrawn participants, you can exclude them from the dataset permanently. $ biobank exclude data/w12345_*.csv removing 20 records from 2 files: - data/w12345_20220101.csv - data/w12345_20220201.csv saving dataset to .biobank/dataset.parquet ---> 100%","title":"Excluding withdrawn participants"},{"location":"tutorial/#extracting-data","text":"There are two ways to work with dataset created by Biobank Tools. You can export the data to a .CSV file You can load the dataset directly in Python","title":"Extracting data"},{"location":"tutorial/#exporting-to-csv","text":"With biobank select command, you can export a subset of the dataset that only contains the fields you're interested in. $ biobank select 2000 . --output data/ukb_cancer.csv ---> 100% writing data/ukb_cancer.csv If you omit the --output option, the biobank select command will output the results to the screen instead. $ biobank select \"2000[67]\" [########################################] | 100% Completed | 9.8s 20006-0.0 20006-0.1 20006-0.2 20006-0.3 20006-0.4 20006-0.5 20006-1.0 20006-1.1 ... 20007-1.3 20007-2.0 20007-2.1 20007-2.2 20007-2.3 20007-2.4 20007-3.0 20007-3.1 eid ... 1000016 1951.771450 1959.965254 1959.758862 2108.331238 1793.185997 2052.090334 ... 1000048 1927.954080 1780.621043 1768.346392 2166.976559 1092.823152 1958.248591 ... 57.135394 52.331757 49.393116 64.879196 68.577423 51.484781 66.384343 1000057 1783.957575 2055.652053 2055.706534 1854.930873 1398.633216 1942.852454 ... 1000059 2036.818137 2222.614293 2239.943216 1932.932354 2801.628978 1920.073145 ... 1000063 2037.248163 2142.236323 2146.492165 2092.512233 1534.692268 1844.065170 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9999968 2175.387378 2169.194698 2177.700380 1996.962135 1942.853501 1916.324173 ... 9999979 1994.938137 1830.922253 1823.217556 1993.885899 1436.625129 1899.035208 ... 9999980 1958.735897 2086.810754 2103.205981 2000.599709 3291.533833 1963.010565 ... 9999986 2002.481976 2039.945909 2038.244240 2068.425323 1250.346010 2066.319885 ... 9999992 1977.196447 1949.820874 1951.181053 1881.749111 2033.254502 1808.468535 ... [599995 rows x 34 columns]","title":"Exporting to .CSV"},{"location":"tutorial/#loading-dataset-in-python","text":"You can load the dataset in Python and apply the same field filters that you would at the command line. from biobank import Dataset dataset = Dataset () cancer_data = dataset . select ( fields = [ \"2000[67]\" ]) print ( cancer_data ) 20006-0.0 20006-0.1 20006-0.2 20006-0.3 20006-0.4 20006-0.5 20006-1.0 20006-1.1 ... 20007-1.3 20007-2.0 20007-2.1 20007-2.2 20007-2.3 20007-2.4 20007-3.0 20007-3.1 eid ... 1000016 1951.771450 1959.965254 1959.758862 2108.331238 1793.185997 2052.090334 ... 1000048 1927.954080 1780.621043 1768.346392 2166.976559 1092.823152 1958.248591 ... 57.135394 52.331757 49.393116 64.879196 68.577423 51.484781 66.384343 1000057 1783.957575 2055.652053 2055.706534 1854.930873 1398.633216 1942.852454 ... 1000059 2036.818137 2222.614293 2239.943216 1932.932354 2801.628978 1920.073145 ... 1000063 2037.248163 2142.236323 2146.492165 2092.512233 1534.692268 1844.065170 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9999968 2175.387378 2169.194698 2177.700380 1996.962135 1942.853501 1916.324173 ... 9999979 1994.938137 1830.922253 1823.217556 1993.885899 1436.625129 1899.035208 ... 9999980 1958.735897 2086.810754 2103.205981 2000.599709 3291.533833 1963.010565 ... 9999986 2002.481976 2039.945909 2038.244240 2068.425323 1250.346010 2066.319885 ... 9999992 1977.196447 1949.820874 1951.181053 1881.749111 2033.254502 1808.468535 ... [599995 rows x 34 columns]","title":"Loading dataset in Python"}]}